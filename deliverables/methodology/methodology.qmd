---
title: "Methodology"
subtitle: "Stack Overflow Question Quality"
author: "Ryan Tobin, James Westbrook"
---
<!---We initially mentioned cross-validation, but I realized that what we were planning to use cross-validation for was unclear. I think it's best to not mention it for now and figure it out later. -->
In our attempt to classify Stack Overflow questions as "good" or "bad" based on their score, we will use a variety of machine learning techniques. We will use logistic regression, (extreme) gradient boosted trees, and SVC. We expect gradient boosting and SVC to achieve a better classification error than logistic regression, however as we are interested in inference, we include it as it is far more interpretable than the other two. Though these methods are quite robust, we expect there to be significant potential for improvement, particularly with how we select our features. We expect that interaction features may play an important role - for example, code to text ratio has been studied as a useful feature. It can be very difficult to identify all of these interaction features.

To evaluate each model, we will use test classification error, and will consider 20% classification error to be successful. Other researchers were able to obtain an accuracy of 80% on similar questions, and obtaining above 80% accuracy would be considered impressive. Along with this, we will compare areas under the ROC curve for each method.

To make sure that we are obtaining legitimate results, we will perform a 70/30% training/test set split for each model. This split will be random for each model so that we can avoid overfitting in our analysis. If we use the same test set for each model, then this may not give a good picture of how each model performs in general, especially with a small test set.

